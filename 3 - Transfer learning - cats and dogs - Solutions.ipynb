{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hk3ZwXYps-pM"
   },
   "source": [
    "This workshop will try to make you realize the impact of transfer learning, applied to images!\n",
    "Let's do some classification over images: cats or dogs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYfoT3wAtdMk"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 535,
     "status": "ok",
     "timestamp": 1702562502729,
     "user": {
      "displayName": "Thomas",
      "userId": "17969775338756094822"
     },
     "user_tz": -60
    },
    "id": "v7YSU88524zg"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy, categorical_accuracy\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from imageio import imread\n",
    "from skimage.transform import resize\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ks5jfNMtXZi"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUtVR1UZ3sq8"
   },
   "source": [
    "Let's start by downloading our example data, a .zip of 2,000 JPG pictures of cats and dogs, and extracting it locally in /tmp.\n",
    "\n",
    "NOTE: The 2,000 images used in this exercise are excerpted from the \"Dogs vs. Cats\" dataset available on Kaggle, which contains 25,000 images. Here, we use a subset of the full dataset to decrease training time for educational purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4035,
     "status": "ok",
     "timestamp": 1702562508327,
     "user": {
      "displayName": "Thomas",
      "userId": "17969775338756094822"
     },
     "user_tz": -60
    },
    "id": "ZAKJPiWY3mZl",
    "outputId": "22956a41-7355-4303-8091-6c03e7e2c096"
   },
   "outputs": [],
   "source": [
    "!wget --no-check-certificate \\\n",
    "    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
    "    -O /tmp/cats_and_dogs_filtered.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 673,
     "status": "ok",
     "timestamp": 1702562510510,
     "user": {
      "displayName": "Thomas",
      "userId": "17969775338756094822"
     },
     "user_tz": -60
    },
    "id": "CLgFf04PwHB0"
   },
   "outputs": [],
   "source": [
    "# Unzip the downloaded foler\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "local_zip = \"/tmp/cats_and_dogs_filtered.zip\"\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('/tmp')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1702562510510,
     "user": {
      "displayName": "Thomas",
      "userId": "17969775338756094822"
     },
     "user_tz": -60
    },
    "id": "k9fX3ONh9YF-"
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "input_shape = (IMG_SIZE, IMG_SIZE)  # Dimensions of the images\n",
    "\n",
    "DATA_DIR = \"/tmp/cats_and_dogs_filtered/\"\n",
    "\n",
    "train_data_dir = DATA_DIR + \"train\"\n",
    "validation_data_dir = DATA_DIR + \"validation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTDakXI13yck"
   },
   "source": [
    "The contents of the .zip are extracted to the base directory /tmp/cats_and_dogs_filtered, which contains train and validation subdirectories for the training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1702562510511,
     "user": {
      "displayName": "Thomas",
      "userId": "17969775338756094822"
     },
     "user_tz": -60
    },
    "id": "e_9t53QN3W8E"
   },
   "outputs": [],
   "source": [
    "#@title display utilities [RUN ME]\n",
    "def plot_examples(path: str):\n",
    "    \"\"\" Plot 10 images found in input path\"\"\"\n",
    "    print(\"Images belonging to class:\", path)\n",
    "    listdir = os.listdir(path)\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=5, figsize=(20, 10))\n",
    "    ax = ax.ravel()\n",
    "    for idx, e in enumerate(listdir[:10]):\n",
    "        img = imread(os.path.join(path, e))\n",
    "        img = resize(img, input_shape)\n",
    "        ax[idx].imshow(img)\n",
    "\n",
    "def plot_training_curve(metric):\n",
    "    plt.plot(range(EPOCHS), history.history[metric], label=f'{metric} training set')\n",
    "    plt.plot(range(EPOCHS), history.history[f\"val_{metric}\"], label=f'{metric} validation set')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnjlI9Fx-GDY"
   },
   "source": [
    "Let's build the datasets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 412,
     "status": "ok",
     "timestamp": 1702562510912,
     "user": {
      "displayName": "Thomas",
      "userId": "17969775338756094822"
     },
     "user_tz": -60
    },
    "id": "FN2GKky_34ii",
    "outputId": "7ffe3db4-5c31-4f36-b35e-f2d25c0d1d7f"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "training_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_data_dir,\n",
    "    labels=\"inferred\",  # The labels will be inferred from the name of the directories /cats/ and /dogs/\n",
    "    label_mode=\"int\",\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    shuffle=True,\n",
    "    seed=123,  # for reproducibility\n",
    "    class_names=[\"cats\", \"dogs\"]  # Must be same as the directories. Is specified in order to choose the order (0=cats, 1=dogs)\n",
    ")\n",
    "training_dataset.repeat()  # Just in case the nb of example is lower than batch size, it will repeat the dataset for 1 epoch.\n",
    "training_dataset = training_dataset.prefetch(tf.data.AUTOTUNE) # prefetch next batch while training (autotune prefetch buffer size)\n",
    "# See doc: https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch\n",
    "\n",
    "validation_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    validation_data_dir,\n",
    "    labels=\"inferred\",  # The labels will be inferred from the name of the directories /cats/ and /dogs/\n",
    "    label_mode=\"int\",\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    shuffle=False,\n",
    "    seed=None,\n",
    "    class_names=[\"cats\", \"dogs\"]  # Must be same as the directories. Is specified in order to choose the order (0=cats, 1=dogs)\n",
    ")\n",
    "validation_dataset = validation_dataset.prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1452,
     "status": "ok",
     "timestamp": 1702562513310,
     "user": {
      "displayName": "Thomas",
      "userId": "17969775338756094822"
     },
     "user_tz": -60
    },
    "id": "TYTyhWNXlSwJ",
    "outputId": "0376e8ad-c583-4a3a-f00c-dcff1eaf9753"
   },
   "outputs": [],
   "source": [
    "for x, y in training_dataset.as_numpy_iterator():\n",
    "    print(x)\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OhZq5Fc-IYf"
   },
   "source": [
    "Let's vizualize some pictures!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QHXKb-6-4F5m",
    "outputId": "a3ea6082-6a45-45ff-87e8-95df1212ab25"
   },
   "outputs": [],
   "source": [
    "plot_examples(DATA_DIR + \"train/cats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "IMsmy8pa4L5p"
   },
   "outputs": [],
   "source": [
    "plot_examples(DATA_DIR + \"train/dogs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zO8g_vtV7eqR"
   },
   "source": [
    "# ðŸ’¡ Model [WORK REQUIRED]\n",
    "1. Start with a dummy single-layer model using one dense layer:\n",
    "\n",
    "* Use a tf.keras.Sequential model. The constructor takes a list of layers.\n",
    "* First, flatten the pixel values of the the input image to a 1D vector so that a dense layer can consume it. The first layer must also specify input shape (hint: size of the image X RGB for red/green/blue)\n",
    "* Add a single dense layer with the appropriate activation and the correct number of units (hint: 2 classes)\n",
    "* Add the last bits and pieces with model.compile(). For a classifier, you need 'sparse_categorical_crossentropy' loss.\n",
    "\n",
    "**==>Train this model: not very good... but all the plumbing is in place.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "w5dIfCmA7fbn"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  pretrained_model,\n",
    "  tf.keras.layers.Flatten(),\n",
    "  # Add some dropout / dense layers\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(units=2, activation='softmax'),\n",
    "])\n",
    "\n",
    "\n",
    "# ðŸ’¡ For fine-tuning a pretrained model, you need a very slow learning rate\n",
    "# and a slow optimizer, like SGD or RMSProp.\n",
    "# optimizer = tf.keras.optimizers.RMSprop(learning_rate=1e-4)\n",
    "# When all freezing, you can use a higher learning rate because no risk\n",
    "# to loose all information already leart by the pretrained model;\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=1e-3)\n",
    "\n",
    "\n",
    "model.compile(\n",
    "  loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Xr_6IwwBYK3y"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True, dpi=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PlAIOZHRwlra"
   },
   "source": [
    "If you're using a pretrained model, check that the number of trainable parameters is 0!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 94837,
     "status": "ok",
     "timestamp": 1701776617612,
     "user": {
      "displayName": "Thomas",
      "userId": "17969775338756094822"
     },
     "user_tz": -60
    },
    "id": "Dgt-epXF767m",
    "outputId": "1f2bd085-db59-432a-f307-c3700eb6ae77"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "history = model.fit(training_dataset, steps_per_epoch=2000//BATCH_SIZE+1, epochs=EPOCHS, validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1701774324193,
     "user": {
      "displayName": "Thomas",
      "userId": "17969775338756094822"
     },
     "user_tz": -60
    },
    "id": "JVsEssAM79RG",
    "outputId": "ae42f068-550d-4a17-eb69-15c8919efafe"
   },
   "outputs": [],
   "source": [
    "plot_training_curve(metric=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "executionInfo": {
     "elapsed": 533,
     "status": "ok",
     "timestamp": 1701774328702,
     "user": {
      "displayName": "Thomas",
      "userId": "17969775338756094822"
     },
     "user_tz": -60
    },
    "id": "hyia9lXgEhHZ",
    "outputId": "74788735-7ee7-46ce-efd0-a7e2c550ae99"
   },
   "outputs": [],
   "source": [
    "plot_training_curve(metric=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUeh1o2K87z0"
   },
   "source": [
    "Let's check some examples predicted as dogs whereas it's cats!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h9WTH6eC6wo7"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict_generator(validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMIof4wMkcUq"
   },
   "source": [
    "Here, we'll display cats wrongly predicted as dogs by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "executionInfo": {
     "elapsed": 6170,
     "status": "ok",
     "timestamp": 1701774504487,
     "user": {
      "displayName": "Thomas",
      "userId": "17969775338756094822"
     },
     "user_tz": -60
    },
    "id": "cZpvpW6r8Wos",
    "outputId": "69ed1bc7-6b89-43d8-82ed-b76ea396527a"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=5, figsize=(20, 10))\n",
    "ax = ax.ravel()\n",
    "fig_idx = 0\n",
    "\n",
    "for idx, prediction in enumerate(predictions[:500]):\n",
    "  if prediction[0] < prediction[1]:  # means that category dog has been predicted.\n",
    "        img = imread(validation_data_dir + \"/cats/\" + os.listdir(validation_data_dir + \"/cats\")[idx])\n",
    "        img = resize(img, input_shape)\n",
    "        if fig_idx < 10:\n",
    "          ax[fig_idx].imshow(img)\n",
    "        fig_idx += 1\n",
    "print(f\"{fig_idx} cats wrongly predicted as dogs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZMPpw9Akh1d"
   },
   "source": [
    "Here, we'll display dogs wrongly predicted as cats by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "executionInfo": {
     "elapsed": 6523,
     "status": "ok",
     "timestamp": 1701774376420,
     "user": {
      "displayName": "Thomas",
      "userId": "17969775338756094822"
     },
     "user_tz": -60
    },
    "id": "HGC9h6KG-srU",
    "outputId": "ccbf844e-da52-4fe2-f664-b1812c59c652"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=5, figsize=(20, 10))\n",
    "ax = ax.ravel()\n",
    "fig_idx = 0\n",
    "\n",
    "for idx, prediction in enumerate(predictions[500:]):\n",
    "  if prediction[0] > prediction[1]:\n",
    "        img = imread(validation_data_dir + \"/dogs/\" + os.listdir(validation_data_dir + \"/dogs\")[idx])\n",
    "        img = resize(img, input_shape)\n",
    "        if fig_idx < 10:\n",
    "          ax[fig_idx].imshow(img)\n",
    "        fig_idx += 1\n",
    "print(f\"{fig_idx} dogs wrongly predicted as cats!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ecqkZsAcl4H9"
   },
   "source": [
    "# Now try transfer learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bf9j6Kv-lY1b"
   },
   "source": [
    "ðŸ’¡ WORK REQUIRED\n",
    "\n",
    "Instead of trying to figure out a better architecture, we will adapt a pretrained model to our data. Please remove all your layers to restart from scratch.\n",
    "\n",
    "* Instantiate a pre-trained model from tf.keras.applications.* You do not need its final softmax layer (include_top=False) because you will be adding your own to fine-tune. The code is already written in the cell below.\n",
    "* Use pretrained_model as your first \"layer\" in your Sequential model.\n",
    "* Follow with Flatten() to turn the data from the pretrained model into a flat 1D vector.\n",
    "* Add your Dense layer with softmax activation and the correct number of units (hint: 2 classes).\n",
    "* You also have to preprocess the datasets according to the pretrained model.\n",
    "\n",
    "**==>Train the model: you should be able to reach above 90% accuracy by training for 5 epochs**\n",
    "\n",
    "\n",
    "This technique is called \"transfer learning\". The pretrained model has been trained on a different dataset but its layers have still learned to recognize bits and pieces of images that can be useful for flowers. You are retraining the last layer only, the pretrained weights are frozen. With far fewer weights to adjust, it works with less data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L7RBv1VJAumL"
   },
   "outputs": [],
   "source": [
    "# each Keras Application expects a specific kind of input preprocessing.\n",
    "# For VGG16, call tf.keras.applications.vgg16.preprocess_input on your inputs before passing them to the model.\n",
    "# Will convert the input images from RGB to BGR, then will zero-center each color channel with respect to the ImageNet dataset, without scaling.\n",
    "for training_images, _ in training_dataset:\n",
    "    training_images = tf.keras.applications.vgg16.preprocess_input(training_images)\n",
    "\n",
    "for val_images, _ in validation_dataset:\n",
    "    val_images = tf.keras.applications.vgg16.preprocess_input(val_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-GPjnRHCl_8K"
   },
   "outputs": [],
   "source": [
    "pretrained_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False ,input_shape=[*input_shape, 3])\n",
    "pretrained_model.trainable = False\n",
    "for layer in pretrained_model.layers:\n",
    "    layer.trainable = False\n",
    "# Check that the number of trainable parameters is empty!\n",
    "assert not pretrained_model.trainable_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2IVuoF7vmM0i"
   },
   "source": [
    "ðŸ’¡ WORK REQUIRED\n",
    "\n",
    "OK, it's working very well! Nice, but maybe it's just because VGG has much more parameters than a simple model.\n",
    "\n",
    "In order to really see the impact of transfer learning, we'll try to train the VGG model from scratch (without any pre-training). You need to initialize its parameters randomly, and make its parameters trainable (see below).\n",
    "Check the results and the impact on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DMT55l20nt68"
   },
   "outputs": [],
   "source": [
    "pretrained_model = tf.keras.applications.VGG16(weights=None, include_top=False ,input_shape=[*input_shape, 3])\n",
    "\n",
    "for layer in pretrained_model.layers:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pva0CaBkqhGR"
   },
   "source": [
    "# Now, let's finetune!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AlE6o0PNqlIX"
   },
   "source": [
    "It's cool to be able to use an existing model, but it's better to fine-tune the hidden layers for the model for our specific use-case. We hope then to get better results.\n",
    "For that, you can unfreeze all the layers of the model, or just the last ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "executionInfo": {
     "elapsed": 1194,
     "status": "ok",
     "timestamp": 1701775718585,
     "user": {
      "displayName": "Thomas",
      "userId": "17969775338756094822"
     },
     "user_tz": -60
    },
    "id": "R7mC44-Fq1oi",
    "outputId": "b9c0180e-c9ef-4f3e-fd0d-c56f775fe6ad"
   },
   "outputs": [],
   "source": [
    "pretrained_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False ,input_shape=[*input_shape, 3])\n",
    "\n",
    "for idx, layer in enumerate(pretrained_model.layers):\n",
    "    if idx >= 11:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "\n",
    "layers = [(layer, layer.name, layer.trainable) for layer in pretrained_model.layers]\n",
    "pd.DataFrame(layers, columns=['Layer Type', 'Layer Name', 'Layer Trainable'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bFy2r9LiBjbT"
   },
   "source": [
    "ðŸ’¡ WORK REQUIRED\n",
    "\n",
    "Try to unfreeze some of the last layers and see what happens!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJZbuX_uCN0w"
   },
   "source": [
    "# Vizualize the convolutions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vLTiEaVICWK7"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1701776746236,
     "user": {
      "displayName": "Thomas",
      "userId": "17969775338756094822"
     },
     "user_tz": -60
    },
    "id": "kmy92ITdCQhw",
    "outputId": "c343a866-b464-4bbd-9f15-eb3f5c85eb31"
   },
   "outputs": [],
   "source": [
    "# summarize filter shapes\n",
    "for layer in pretrained_model.layers:\n",
    "\t# check for convolutional layer\n",
    "\tif 'conv' not in layer.name:\n",
    "\t\tcontinue\n",
    "\t# get filter weights\n",
    "\tfilters, biases = layer.get_weights()\n",
    "\tprint(layer.name, filters.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCoUyu5cCfXC"
   },
   "source": [
    "We can see that all convolutional layers use 3Ã—3 filters, which are small and perhaps easy to interpret.\n",
    "\n",
    "An architectural concern with a convolutional neural network is that the depth of a filter must match the depth of the input for the filter (e.g. the number of channels).\n",
    "\n",
    "We can see that for the input image with three channels for red, green and blue, that each filter has a depth of three (here we are working with a channel-last format). We could visualize one filter as a plot with three images, one for each channel, or compress all three down to a single color image, or even just look at the first channel and assume the other channels will look the same. The problem is, we then have 63 other filters that we might like to visualize.\n",
    "\n",
    "We can retrieve the filters from the first layer as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ByVFtwyChSL"
   },
   "outputs": [],
   "source": [
    "layer = pretrained_model.get_layer('block1_conv1')\n",
    "filters, biases = layer.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1S96fJHsCjUX"
   },
   "source": [
    "Now we can enumerate the first six filters out of the 64 in the block and plot each of the three channels of each filter.\n",
    "\n",
    "We use the matplotlib library and plot each filter as a new row of subplots, and each filter channel or depth as a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1254,
     "status": "ok",
     "timestamp": 1701777847646,
     "user": {
      "displayName": "Thomas",
      "userId": "17969775338756094822"
     },
     "user_tz": -60
    },
    "id": "4pp7rzoLClfk",
    "outputId": "3c3b42c2-cdb6-4927-defd-c2f489b5e723"
   },
   "outputs": [],
   "source": [
    "f_min, f_max = filters.min(), filters.max()\n",
    "\n",
    "f = pyplot.figure(figsize=(16,16))\n",
    "# plot first few filters\n",
    "n_filters, ix = 6, 1\n",
    "for i in range(n_filters):\n",
    "\t# get the filter\n",
    "\tf = filters[:, :, :, i]\n",
    "\t# plot each channel separately\n",
    "\tfor j in range(3):\n",
    "\t\t# specify subplot and turn of axis\n",
    "\t\tax = pyplot.subplot(n_filters, 3, ix)\n",
    "\t\tax.set_xticks([])\n",
    "\t\tax.set_yticks([])\n",
    "\t\t# plot filter channel on a two-color scale,\n",
    "\t\t# from red (small, negative) to blue (large, positive)\n",
    "\t\tpyplot.imshow(f[:, :, j], vmin=f_min, vmax=f_max, cmap='RdBu')\n",
    "\t\tix += 1\n",
    "# show the figure\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzHEeev7l9N1"
   },
   "source": [
    "# Some insights on the solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRyvv6HVmBST"
   },
   "source": [
    "You should notice that using a pretrained model by freezing all the weights is the best solution. Indeed, the task here is so related to the ImageNet dataset that trying to fine-tune is too risky, and you may loose some information.\n",
    "\n",
    "During finetuning, we already have a model which is very good so we donâ€™t want to change the weights too much. So, we would use an optimizer with a very slow learning rate. In general, SGD is good choice for this as opposed to adaptive methods like Adam etc. (lr=1e-4).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "",
   "provenance": [
    {
     "file_id": "1YbU0Sajr6TU2MXff-dxYPsqv25Uk6zap",
     "timestamp": 1701769510551
    }
   ],
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
